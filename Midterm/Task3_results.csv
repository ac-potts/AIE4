,question,contexts,answer,ground_truth,faithfulness,answer_relevancy,context_recall,context_precision,answer_correctness
0,What criteria are used to measure AI system performance or assurance in deployment settings?,"['and Homogenization \nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, End-Users, Operation and Monitoring, TEVV \n \nMEASURE 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are \ninformed by input from domain experts and relevant AI Actors to validate whether the system is performing consistently as \nintended. Results are documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-4.2-001'
 '31 \nMS-2.3-004 \nUtilize a purpose-built testing environment such as NIST Dioptra to empirically \nevaluate GAI trustworthy characteristics. \nCBRN Information or Capabilities; \nData Privacy; Confabulation; \nInformation Integrity; Information \nSecurity; Dangerous, Violent, or \nHateful Content; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, TEVV \n \nMEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the'
 'content back to individual human subjects. \nData Privacy; Human-AI \nConﬁguration \nAI Actor Tasks: AI Development, Human Factors, TEVV \n \nMEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for \nconditions similar to deployment setting(s). Measures are documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.3-001 Consider baseline model performance on suites of benchmarks when selecting a'
 '32 \nMEASURE 2.6: The AI system is evaluated regularly for safety risks – as identiﬁed in the MAP function. The AI system to be \ndeployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if \nmade to operate beyond its knowledge limits. Safety metrics reﬂect system reliability and robustness, real-time monitoring, and \nresponse times for AI system failures. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.6-001']",AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented.,AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented.,1.0,0.9580682179411465,1.0,0.999999999975,1.0
1,What applications can GAI technologies be leveraged for?,"['assessments, and alerting, dynamic risk assessments, and real-time reporting \ntools for monitoring third-party GAI risks; Consider policy adjustments across GAI \nmodeling libraries, tools and APIs, ﬁne-tuned models, and embedded tools; \nAssess GAI vendors, open-source or proprietary GAI tools, or GAI service \nproviders against incident or vulnerability databases. \nData Privacy; Human-AI \nConﬁguration; Information \nSecurity; Intellectual Property; \nValue Chain and Component'
 'applications and contexts of use. These can include data labeling and preparation, development of GAI \nmodels, content moderation, code generation and review, text generation and editing, image and video \ngeneration, summarization, search, and chat. These activities can take place within organizational \nsettings or in the public domain. \nOrganizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that conﬂict'
 'conﬁgurations in order to manage their risks eﬀectively. Organizations’ use of GAI systems may also \nwarrant additional human review, tracking and documentation, and greater management oversight.  \nAI technology can produce varied outputs in multiple modalities and present many classes of user \ninterfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely diﬀering'
 'with their tolerances or values. Governance tools and protocols that are applied to other types of AI \nsystems can be applied to GAI systems. These plans and actions include: \n• Accessibility and reasonable \naccommodations \n• AI actor credentials and qualiﬁcations  \n• Alignment to organizational values \n• Auditing and assessment \n• Change-management controls \n• Commercial use \n• Data provenance']","GAI technologies can be leveraged for applications such as data labeling and preparation, development of GAI models, content moderation, code generation and review, text generation and editing, image and video generation, summarization, search, and chat.",GAI technologies can be leveraged for many applications such as content generation and synthetic data.,1.0,0.9680357674213583,1.0,0.49999999995,0.36254778697405043
2,What was the purpose of the meetings conducted by OSTP regarding the AI Bill of Rights?,"['APPENDIX\n• OSTP conducted meetings with a variety of stakeholders in the private sector and civil society. Some of these\nmeetings were specifically focused on providing ideas related to the development of the Blueprint for an AI\nBill of Rights while others provided useful general context on the positive use cases, potential harms, and/or\noversight possibilities for these technologies. Participants in these conversations from the private sector and\ncivil society included:\nAdobe'
 'online at: https://www.whitehouse.gov/ostp/ai-bill-of-rights \nAbout the Office of Science and Technology Policy \nThe Office of Science and Technology Policy (OSTP) was established by the National Science and Technology \nPolicy, Organization, and Priorities Act of 1976 to provide the President and others within the Executive Office \nof the President with advice on the scientific, engineering, and technological aspects of the economy, national'
 'About this Document \nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People was \npublished by the White House Office of Science and Technology Policy in October 2022. This framework was \nreleased one year after OSTP announced the launch of a process to develop “a bill of rights for an AI-powered \nworld.” Its release follows a year of public engagement to inform this initiative. The framework is available'
 'ing sessions, meetings, a formal request for information, and input to a publicly accessible and widely-publicized \nemail address, people throughout the United States, public servants across Federal agencies, and members of the \ninternational community spoke up about both the promises and potential harms of these technologies, and \nplayed a central role in shaping the Blueprint for an AI Bill of Rights. The core messages gleaned from these']","The purpose of the meetings conducted by OSTP regarding the AI Bill of Rights was to provide ideas related to the development of the Blueprint for an AI Bill of Rights, as well as to gather useful general context on the positive use cases, potential harms, and oversight possibilities for these technologies.","The purpose of the meetings conducted by OSTP regarding the AI Bill of Rights was to provide ideas related to the development of the Blueprint for an AI Bill of Rights and to offer useful general context on the positive use cases, potential harms, and/or oversight possibilities for these technologies.",1.0,0.9999999999999996,1.0,0.8055555555287036,0.9996842476789078
3,What are the intellectual property risks associated with GAI systems?,"['2.10. \nIntellectual Property \nIntellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair \nuse under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI \noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \ncopyright. \nHow GAI relates to copyright, including the status of generated content that is similar to but does not'
 'GAI resources; Apply organizational risk tolerances to ﬁne-tuned third-party \nmodels; Apply organizational risk tolerance to existing third-party models \nadapted to a new domain; Reassess risk measurements after ﬁne-tuning third-\nparty GAI models. \nValue Chain and Component \nIntegration; Intellectual Property \nMG-3.1-002 \nTest GAI system value chain risks (e.g., data poisoning, malware, other software \nand hardware vulnerabilities; labor practices; data privacy and localization'
 'assessments, and alerting, dynamic risk assessments, and real-time reporting \ntools for monitoring third-party GAI risks; Consider policy adjustments across GAI \nmodeling libraries, tools and APIs, ﬁne-tuned models, and embedded tools; \nAssess GAI vendors, open-source or proprietary GAI tools, or GAI service \nproviders against incident or vulnerability databases. \nData Privacy; Human-AI \nConﬁguration; Information \nSecurity; Intellectual Property; \nValue Chain and Component'
 'Suggested Action \nGAI Risks \nMP-2.2-001 \nIdentify and document how the system relies on upstream data sources, \nincluding for content provenance, and if it serves as an upstream dependency for \nother systems. \nInformation Integrity; Value Chain \nand Component Integration \nMP-2.2-002 \nObserve and analyze how the GAI system interacts with external networks, and \nidentify any potential for negative externalities, particularly where content \nprovenance might be compromised. \nInformation Integrity']","Intellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair use under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI outputs displaying instances of training data memorization could infringe on copyright.","Intellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair use under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI outputs displaying instances of training data memorization could infringe on copyright. The status of generated content that is similar to but does not strictly copy work protected by copyright is currently being debated in legal fora. Similar discussions are taking place regarding the use or emulation of personal identity, likeness, or voice without permission.",1.0,0.9915799450301824,0.5,0.999999999975,0.7472456501688505
4,What measures are suggested for ensuring the accuracy and integrity of information generated by GAI systems?,"['Suggested Action \nGAI Risks \nMP-2.2-001 \nIdentify and document how the system relies on upstream data sources, \nincluding for content provenance, and if it serves as an upstream dependency for \nother systems. \nInformation Integrity; Value Chain \nand Component Integration \nMP-2.2-002 \nObserve and analyze how the GAI system interacts with external networks, and \nidentify any potential for negative externalities, particularly where content \nprovenance might be compromised. \nInformation Integrity'
 'Suggested Action \nGAI Risks \nMS-4.2-001 \nConduct adversarial testing at a regular cadence to map and measure GAI risks, \nincluding tests to address attempts to deceive or manipulate the application of \nprovenance techniques or other misuses. Identify vulnerabilities and \nunderstand potential misuse scenarios and unintended outputs. \nInformation Integrity; Information \nSecurity \nMS-4.2-002 \nEvaluate GAI system performance in real-world scenarios to observe its'
 'Information Integrity \nAI Actor Tasks: End Users \n \nMAP 2.3: Scientiﬁc integrity and TEVV considerations are identiﬁed and documented, including those related to experimental \ndesign, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct \nvalidation \nAction ID \nSuggested Action \nGAI Risks \nMP-2.3-001 \nAssess the accuracy, quality, reliability, and authenticity of GAI output by'
 'transparency. \nInformation Integrity \nMP-3.4-003 Develop certiﬁcation programs that test proﬁciency in managing GAI risks and \ninterpreting content provenance, relevant to speciﬁc industry and context. \nInformation Integrity \nMP-3.4-004 Delineate human proﬁciency tests from tests of GAI capabilities. \nHuman-AI Conﬁguration \nMP-3.4-005 Implement systems to continually monitor and track the outcomes of human-GAI \nconﬁgurations for future reﬁnement and improvements. \nHuman-AI Conﬁguration;']","The suggested measures for ensuring the accuracy and integrity of information generated by GAI systems include:

1. Assessing the accuracy, quality, reliability, and authenticity of GAI output (MP-2.3-001).
2. Conducting adversarial testing at a regular cadence to map and measure GAI risks, including addressing attempts to deceive or manipulate the application of provenance techniques (MS-4.2-001).
3. Evaluating GAI system performance in real-world scenarios to observe its performance (as indicated in the context but not fully detailed).

Additionally, developing certification programs that test proficiency in managing GAI risks and interpreting content provenance is suggested (MP-3.4-003).","The suggested measures for ensuring the accuracy and integrity of information generated by GAI systems include reviewing and documenting the accuracy, representativeness, relevance, and suitability of data used at different stages of the AI life cycle; deploying fact-checking techniques to verify the accuracy and veracity of information; developing and implementing testing techniques to identify GAI produced content that might be indistinguishable from human-generated content; and implementing plans for regular adversarial testing to identify vulnerabilities and potential manipulation or misuse.",1.0,1.0000000000000004,0.5,0.999999999975,0.3621761594931827
5,What measures are suggested to manage GAI risks associated with third-party resources?,"['GAI resources; Apply organizational risk tolerances to ﬁne-tuned third-party \nmodels; Apply organizational risk tolerance to existing third-party models \nadapted to a new domain; Reassess risk measurements after ﬁne-tuning third-\nparty GAI models. \nValue Chain and Component \nIntegration; Intellectual Property \nMG-3.1-002 \nTest GAI system value chain risks (e.g., data poisoning, malware, other software \nand hardware vulnerabilities; labor practices; data privacy and localization'
 'out by employees or third parties. Many of the actions cited above are relevant and options for \naddressing third-party considerations. \nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \nthe collection and use of third-party data for model inputs. Organizations may consider varying risk'
 'Harmful Bias Managed, Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable \n3. \nSuggested Actions to Manage GAI Risks \nThe following suggested actions target risks unique to or exacerbated by GAI. \nIn addition to the suggested actions below, AI risk management activities and actions set forth in the AI \nRMF 1.0 and Playbook are already applicable for managing GAI risks. Organizations are encouraged to'
 'assessments, and alerting, dynamic risk assessments, and real-time reporting \ntools for monitoring third-party GAI risks; Consider policy adjustments across GAI \nmodeling libraries, tools and APIs, ﬁne-tuned models, and embedded tools; \nAssess GAI vendors, open-source or proprietary GAI tools, or GAI service \nproviders against incident or vulnerability databases. \nData Privacy; Human-AI \nConﬁguration; Information \nSecurity; Intellectual Property; \nValue Chain and Component']","The suggested measures to manage GAI risks associated with third-party resources include applying organizational risk tolerances to fine-tuned third-party models, reassessing risk measurements after fine-tuning these models, and testing GAI system value chain risks such as data poisoning, malware, and other vulnerabilities. Additionally, organizations should consider clear guidelines for transparency and risk management regarding the collection and use of third-party data for model inputs, and they may assess GAI vendors and tools against incident or vulnerability databases.","The suggested measures to manage GAI risks associated with third-party resources include applying organizational risk tolerances and controls to third-party GAI resources, testing GAI system value chain risks, re-assessing model risks after fine-tuning or retrieval-augmented generation implementation, and taking reasonable measures to review training data for sensitive information and intellectual property.",1.0,0.9999985038243061,0.75,0.999999999975,0.5778771192081736
6,What measures are suggested to ensure information integrity in the context of AI systems?,"['or risks of bias or homogenization. \nThere may also be concerns about emotional entanglement between humans and GAI systems, which \ncould lead to negative psychological impacts. \nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with \nHarmful Bias Managed, Privacy Enhanced, Safe, Valid and Reliable \n2.8. Information Integrity \nInformation integrity describes the “spectrum of information and associated patterns of its creation,'
 'Information Integrity; Harmful Bias \nand Homogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End-Users, Operation and Monitoring, TEVV \n \nMEASURE 2.10: Privacy risk of the AI system – as identiﬁed in the MAP function – is examined and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.10-001 \nConduct AI red-teaming to assess issues such as: Outputting of training data \nsamples, and subsequent reverse engineering, model extraction, and'
 '41 \nMG-2.2-006 \nUse feedback from internal and external AI Actors, users, individuals, and \ncommunities, to assess impact of AI-generated content. \nHuman-AI Conﬁguration \nMG-2.2-007 \nUse real-time auditing tools where they can be demonstrated to aid in the \ntracking and validation of the lineage and authenticity of AI-generated data. \nInformation Integrity \nMG-2.2-008 \nUse structured feedback mechanisms to solicit and capture user input about AI-'
 'Information Integrity  \nMP-2.3-004 Develop and implement testing techniques to identify GAI produced content (e.g., \nsynthetic media) that might be indistinguishable from human-generated content. Information Integrity \nMP-2.3-005 Implement plans for GAI systems to undergo regular adversarial testing to identify \nvulnerabilities and potential manipulation or misuse. \nInformation Security \nAI Actor Tasks: AI Development, Domain Experts, TEVV']","The suggested measures to ensure information integrity in the context of AI systems include:

1. Use feedback from internal and external AI Actors, users, individuals, and communities to assess the impact of AI-generated content.
2. Use real-time auditing tools where they can aid in tracking and validating the lineage and authenticity of AI-generated data.
3. Use structured feedback mechanisms to solicit and capture user input about AI-generated content.
4. Develop and implement testing techniques to identify GAI produced content (e.g., synthetic media) that might be indistinguishable from human-generated content.
5. Implement plans for GAI systems to undergo regular adversarial testing to identify vulnerabilities and potential manipulation or misuse.","Suggested measures to ensure information integrity in the context of AI systems include employing methods to trace the origin and modifications of digital content, integrating tools designed to analyze content provenance and detect data anomalies, verifying the authenticity of digital signatures, and identifying patterns associated with misinformation or manipulation. Additionally, it is recommended to disaggregate evaluation metrics by demographic factors to identify discrepancies in how content provenance mechanisms work across diverse populations.",1.0,0.99140249200088,0.0,0.4166666666458333,0.6979743619121211
7,What are the key considerations regarding data privacy in the context of the AI Bill of Rights?,"['Applying The Blueprint for an AI Bill of Rights \nSENSITIVE DATA: Data and metadata are sensitive if they pertain to an individual in a sensitive domain \n(defined below); are generated by technologies used in a sensitive domain; can be used to infer data from a \nsensitive domain or sensitive data about an individual (such as disability-related data, genomic data, biometric \ndata, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationship'
 'the principles described in the Blueprint for an AI Bill of Rights may be necessary to comply with existing law, \nconform to the practicalities of a specific use case, or balance competing public interests. In particular, law \nenforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties, \nand privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed in'
 'civil rights, civil liberties, and privacy. The Blueprint for an AI Bill of Rights includes this Foreword, the five \nprinciples, notes on Applying the The Blueprint for an AI Bill of Rights, and a Technical Companion that gives \nconcrete steps that can be taken by many kinds of organizations—from governments at all levels to companies of \nall sizes—to uphold these values. Experts from across the private sector, governments, and international'
 'automated systems in certain settings such as AI systems used as part of school building security or automated \nhealth diagnostic systems. \nThe Blueprint for an AI Bill of Rights recognizes that law enforcement activities require a balancing of \nequities, for example, between the protection of sensitive law enforcement information and the principle of \nnotice; as such, notice may not be appropriate, or may need to be adjusted to protect sources, methods, and']","The key considerations regarding data privacy in the context of the AI Bill of Rights include recognizing that sensitive data and metadata are considered sensitive if they pertain to individuals in sensitive domains, are generated by technologies used in those domains, or can be used to infer sensitive information about individuals. Additionally, there is a need to balance civil rights, civil liberties, and privacy, especially in law enforcement and regulatory contexts, where government actors must protect these rights using mechanisms that may differ from the principles discussed in the Blueprint.",The answer to given question is not present in context,1.0,1.0,1.0,0.0,0.1794624337519387
8,What are some examples of how data privacy principles aim to protect against identity theft?,"[""DATA PRIVACY \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \n•\nAn insurer might collect data from a person's social media presence as part of deciding what life\ninsurance rates they should be offered.64\n•\nA data broker harvested large amounts of personal data and then suffered a breach, exposing hundreds of""
 'DATA PRIVACY \nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\nReal-life examples of how these principles can become reality, through laws, policies, and practical \ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \nThe Privacy Act of 1974 requires privacy protections for personal information in federal'
 'DATA PRIVACY \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \nData privacy is a foundational and cross-cutting principle required for achieving all others in this framework. Surveil\xad\nlance and data collection, sharing, use, and reuse now sit at the foundation of business models across many industries,'
 'Increasingly, some companies are taking these concerns seriously and integrating mechanisms to protect consumer \nprivacy into their products by design and by default, including by minimizing the data they collect, communicating \ncollection and use clearly, and improving security practices. Federal government surveillance and other collection and \nuse of data is governed by legal protections that help to protect civil liberties and provide for limits on data retention']",I don't know.,"Some examples of how data privacy principles aim to protect against identity theft include: a data broker harvesting large amounts of personal data and suffering a breach that exposes individuals to potential identity theft, and an insurer collecting data from a person's social media presence to determine life insurance rates, which could lead to misuse of personal information.",0.0,0.0,1.0,0.9999999999,0.17761643798106413
9,What organizational practices are necessary for enabling AI testing and incident identification?,"['GOVERN 4.3: Organizational practices are in place to enable AI testing, identiﬁcation of incidents, and information sharing. \nAction ID \nSuggested Action \nGAI Risks \nGV4.3--001 \nEstablish policies for measuring the eﬀectiveness of employed content \nprovenance methodologies (e.g., cryptography, watermarking, steganography, \netc.) \nInformation Integrity \nGV-4.3-002 \nEstablish organizational practices to identify the minimum set of criteria'
 'and implement measures to prevent similar ones in the future, organizations could consider developing \nguidelines for publicly available incident reporting which include information about AI actor \nresponsibilities. These guidelines would help AI system operators identify GAI incidents across the AI \nlifecycle and with AI Actors regardless of role. Documentation and review of third-party inputs and'
 'database, AVID, CVE, NVD, or OECD AI incident monitor). \nHuman-AI Conﬁguration; Value \nChain and Component Integration \nGV-2.1-002 Establish procedures to engage teams for GAI system incident response with \ndiverse composition and responsibilities based on the particular incident type. \nHarmful Bias and Homogenization \nGV-2.1-003 Establish processes to verify the AI Actors conducting GAI incident response tasks \ndemonstrate and maintain the appropriate skills and training.'
 'sense of AI-generated information, and subsequent actions and eﬀects, including UX, usability, \nand other structured, randomized experiments.  \n• \nAI Red-teaming: A structured testing exercise used to probe an AI system to ﬁnd ﬂaws and \nvulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled \nenvironment and in collaboration with system developers. \nInformation gathered from structured public feedback can inform design, implementation, deployment']","Organizational practices necessary for enabling AI testing and incident identification include establishing policies for measuring the effectiveness of employed content provenance methodologies, and establishing organizational practices to identify the minimum set of criteria for incident identification and information sharing.","Organizational practices necessary for enabling AI testing and incident identification include establishing policies for measuring the effectiveness of employed content provenance methodologies and identifying the minimum set of criteria necessary for GAI system incident reporting, such as System ID, Title, Reporter, System/Source, Data Reported, Date of Incident, Description, Impact(s), and Stakeholder(s) Impacted.",1.0,1.0,0.6666666666666666,0.999999999975,0.8430645943068206
10,"What to consider for GAI's enviro sustainability and social impact, especially on data diversity and community input?","['impacted communities. \nEnvironmental; Harmful Bias and \nHomogenization \nMS-2.11-004 \nReview, document, and measure sources of bias in GAI training and TEVV data: \nDiﬀerences in distributions of outcomes across and within groups, including \nintersecting groups; Completeness, representativeness, and balance of data \nsources; demographic group and subgroup coverage in GAI system training \ndata; Forms of latent systemic bias in images, text, audio, embeddings, or other'
 '20 \nGV-4.3-003 \nVerify information sharing and feedback mechanisms among individuals and \norganizations regarding any negative impact from GAI systems. \nInformation Integrity; Data \nPrivacy \nAI Actor Tasks: AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \n \nGOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those'
 'out by employees or third parties. Many of the actions cited above are relevant and options for \naddressing third-party considerations. \nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \nthe collection and use of third-party data for model inputs. Organizations may consider varying risk'
 '29 \nMS-1.1-006 \nImplement continuous monitoring of GAI system impacts to identify whether GAI \noutputs are equitable across various sub-populations. Seek active and direct \nfeedback from aﬀected communities via structured feedback mechanisms or red-\nteaming to monitor and improve outputs.  \nHarmful Bias and Homogenization \nMS-1.1-007 \nEvaluate the quality and integrity of data used in training and the provenance of \nAI-generated content, for example by employing techniques like chaos']","To consider GAI's environmental sustainability and social impact, especially on data diversity and community input, one should review, document, and measure sources of bias in GAI training and testing data. This includes assessing the differences in distributions of outcomes across and within groups, ensuring completeness, representativeness, and balance of data sources, and verifying demographic group and subgroup coverage in GAI system training data. Additionally, implementing continuous monitoring of GAI system impacts is crucial to identify whether outputs are equitable across various sub-populations and seeking active feedback from affected communities through structured mechanisms or red-teaming to monitor and improve outputs.","For GAI's environmental sustainability and social impact, it is important to assess the proportion of synthetic to non-synthetic training data to ensure it is not overly homogenous or GAI-produced. Additionally, documenting anticipated environmental impacts of model development, maintenance, and deployment is crucial. Gathering structured feedback about content provenance from operators, users, and impacted communities is also essential to actively seek feedback on generated content quality and potential biases, thereby ensuring diverse and inclusive content generation.",0.8235294117647058,0.9325665121515332,0.25,0.9166666666361111,0.7872267226117953
11,What criteria for escalating GAI incidents to risk management for safe decommissioning?,"['42 \nMG-2.4-002 \nEstablish and maintain procedures for escalating GAI system incidents to the \norganizational risk management authority when speciﬁc criteria for deactivation \nor disengagement is met for a particular context of use or for the GAI system as a \nwhole. \nInformation Security \nMG-2.4-003 \nEstablish and maintain procedures for the remediation of issues which trigger \nincident response processes for the use of a GAI system, and provide stakeholders'
 '17 \nGOVERN 1.7: Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that \ndoes not increase risks or decrease the organization’s trustworthiness. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.7-001 Protocols are put in place to ensure GAI systems are able to be deactivated when \nnecessary.  \nInformation Security; Value Chain \nand Component Integration \nGV-1.7-002 \nConsider the following factors when decommissioning GAI systems: Data'
 'timelines associated with the remediation plan. \nInformation Security \n \nMG-2.4-004 Establish and regularly review speciﬁc criteria that warrants the deactivation of \nGAI systems in accordance with set risk tolerances and appetites. \nInformation Security \n \nAI Actor Tasks: AI Deployment, Governance and Oversight, Operation and Monitoring \n \nMANAGE 3.1: AI risks and beneﬁts from third-party resources are regularly monitored, and risk controls are applied and \ndocumented. \nAction ID'
 '15 \nGV-1.3-004 Obtain input from stakeholder communities to identify unacceptable use, in \naccordance with activities in the AI RMF Map function. \nCBRN Information or Capabilities; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias \nand Homogenization; Dangerous, \nViolent, or Hateful Content \nGV-1.3-005 \nMaintain an updated hierarchy of identiﬁed and expected GAI risks connected to \ncontexts of GAI model advancement and use, potentially including specialized risk']",The criteria for escalating GAI incidents to the organizational risk management authority for safe decommissioning include the specific criteria for deactivation or disengagement that are met for a particular context of use or for the GAI system as a whole.,The criteria for escalating GAI incidents to the organizational risk management authority include specific criteria for deactivation or disengagement that are met for a particular context of use or for the GAI system as a whole.,0.5,0.9355358759845149,1.0,0.999999999975,0.9943495331217282
12,What studies aim to improve transparency in automated systems affecting rights?,"['transparency to the public in the mitigation of such biases. \nBut there is much more work to do to protect the public from algorithmic discrimination to use and design \nautomated systems in an equitable way. The guardrails protecting the public from discrimination in their daily \nlives should include their digital lives and impacts—basic safeguards against abuse, bias, and discrimination to'
 'Major technology companies are piloting new ways to communicate with the public about \ntheir automated technologies. For example, a collection of non-profit organizations and companies have \nworked together to develop a framework that defines operational approaches to transparency for machine \nlearning systems.88 This framework, and others like it,89 inform the public about the use of these tools, going'
 'biased data and discriminatory outcomes, opaque decision-making processes, and lack of public trust and \nunderstanding of the algorithmic systems. The interventions and key needs various panelists put forward as \nnecessary to the future design of critical AI systems included ongoing transparency, value sensitive and \nparticipatory design, explanations designed for relevant stakeholders, and public consultation. \nVarious'
 'independent evaluation of potential algorithmic discrimination caused by automated systems they use or \noversee. In the case of public sector uses, these independent evaluations should be made public unless law \nenforcement or national security restrictions prevent doing so. Care should be taken to balance individual \nprivacy with evaluation data access needs; in many cases, policy-based and/or technological innovations and \ncontrols allow access to such data without compromising privacy.']",I don't know.,"The National Institute of Standards and Technology (NIST) is conducting fundamental research on the explainability of AI systems, while the Defense Advanced Research Projects Agency (DARPA) has a program on Explainable Artificial Intelligence aimed at creating machine learning techniques that produce more explainable models. Additionally, the National Science Foundation's program on Fairness in Artificial Intelligence includes research foundations for explainable AI.",0.0,0.0,0.0,0.999999999975,0.18067979929441988
13,What strategies are in place to boost consumer data privacy amid rising surveillance in the U.S.?,"['Increasingly, some companies are taking these concerns seriously and integrating mechanisms to protect consumer \nprivacy into their products by design and by default, including by minimizing the data they collect, communicating \ncollection and use clearly, and improving security practices. Federal government surveillance and other collection and \nuse of data is governed by legal protections that help to protect civil liberties and provide for limits on data retention'
 'with more and more companies tracking the behavior of the American public, building individual profiles based on \nthis data, and using this granular-level information as input into automated systems that further track, profile, and \nimpact the American public. Government agencies, particularly law enforcement agencies, also use and help develop \na variety of technologies that enhance and expand surveillance capabilities, which similarly collect data used as input'
 'potential impact of surveillance technologies on your rights, opportunities, or \naccess. \nDATA PRIVACY\n30'
 'in some cases. Many states have also enacted consumer data privacy protection regimes to address some of these \nharms. \nHowever, these are not yet standard practices, and the United States lacks a comprehensive statutory or regulatory \nframework governing the rights of the public when it comes to personal data. While a patchwork of laws exists to \nguide the collection and use of personal data in specific contexts, including health, employment, education, and credit,']","Some companies are integrating mechanisms to protect consumer privacy into their products by design and by default. This includes minimizing the data they collect, clearly communicating the collection and use of data, and improving security practices. Additionally, many states have enacted consumer data privacy protection regimes to address some harms related to data collection and use.","Some companies are taking concerns about consumer data privacy seriously by integrating mechanisms to protect privacy into their products by design and by default. This includes minimizing the data they collect, clearly communicating data collection and use, and improving security practices. Additionally, federal government surveillance and data collection is governed by legal protections that help protect civil liberties and provide limits on data retention in some cases. Many states have also enacted consumer data privacy protection regimes to address some of these harms.",1.0,0.9050546717916021,1.0,0.9166666666361111,0.8049292473907331
14,What steps can ensure safe deployment of automated systems with community input?,"['maintenance phases of automated system development, with emphasis on early-stage consultation before a \nsystem is introduced or a large change implemented. This consultation should directly engage diverse impact\xad\ned communities to consider concerns and risks that may be unique to those communities, or disproportionate\xad\nly prevalent or severe for them. The extent of this engagement and the form of outreach to relevant stakehold\xad'
 'SAFE AND EFFECTIVE SYSTEMS \nYou should be protected from unsafe or ineffective sys\xad\ntems. Automated systems should be developed with consultation \nfrom diverse communities, stakeholders, and domain experts to iden\xad\ntify concerns, risks, and potential impacts of the system. Systems \nshould undergo pre-deployment testing, risk identification and miti\xad\ngation, and ongoing monitoring that demonstrate they are safe and \neffective based on their intended use, mitigation of unsafe outcomes'
 'communities that may not be direct users of the automated system, risks resulting from purposeful misuse of \nthe system, and other concerns identified via the consultation process. Assessment and, where possible, mea\xad\nsurement of the impact of risks should be included and balanced such that high impact risks receive attention \nand mitigation proportionate with those impacts. Automated systems with the intended purpose of violating'
 'dures before deploying the system, as well as responsibility of specific individuals or entities to oversee ongoing \nassessment and mitigation. Organizational stakeholders including those with oversight of the business process \nor operation being automated, as well as other organizational divisions that may be affected due to the use of \nthe system, should be involved in establishing governance procedures. Responsibility should rest high enough']","To ensure safe deployment of automated systems with community input, the following steps can be taken:

1. **Early-stage Consultation**: Engage diverse impacted communities before a system is introduced or significant changes are implemented to consider unique concerns and risks.

2. **Involvement of Stakeholders**: Consult with stakeholders and domain experts to identify concerns, risks, and potential impacts of the system.

3. **Pre-deployment Testing**: Conduct thorough pre-deployment testing to ensure the system is safe and effective based on its intended use.

4. **Risk Identification and Mitigation**: Identify and mitigate risks associated with the automated system.

5. **Ongoing Monitoring**: Implement ongoing monitoring to demonstrate that the system remains safe and effective after deployment.

6. **Assessment of Impact**: Assess and, where possible, measure the impact of risks, ensuring that high-impact risks receive appropriate attention and mitigation.

7. **Governance Procedures**: Establish governance procedures involving organizational stakeholders who oversee the business process or operation being automated.

By following these steps, organizations can ensure that automated systems are developed and deployed safely with input from affected communities.","To ensure safe deployment of automated systems with community input, the following steps can be taken: 1. Consultation with the public during the design, implementation, deployment, acquisition, and maintenance phases, emphasizing early-stage engagement with diverse impacted communities. 2. Extensive testing of systems before deployment, following domain-specific best practices and mirroring real-world conditions. 3. Identification and mitigation of potential risks before and during deployment, focusing on meaningful impacts on people's rights and opportunities. 4. Ongoing monitoring and evaluation to confirm safety and effectiveness, including independent reporting of steps taken to mitigate potential harms.",1.0,0.9941017901106489,1.0,0.999999999975,0.5216677366576475
15,How does tracking AI content's origin help with authenticity and trust?,"['history of content. Provenance data tracking and synthetic content detection can help facilitate greater \ninformation access about both authentic and synthetic content to users, enabling better knowledge of \ntrustworthiness in AI systems. When combined with other organizational accountability mechanisms, \ndigital content transparency approaches can enable processes to trace negative outcomes back to their'
 'A.1.6. Content Provenance \nOverview \nGAI technologies can be leveraged for many applications such as content generation and synthetic data. \nSome aspects of GAI outputs, such as the production of deepfake content, can challenge our ability to \ndistinguish human-generated content from AI-generated synthetic content. To help manage and mitigate \nthese risks, digital transparency mechanisms like provenance data tracking can trace the origin and'
 'source, improve information integrity, and uphold public trust. Provenance data tracking and synthetic \ncontent detection mechanisms provide information about the origin and history of content to assist in \nGAI risk management eﬀorts. \nProvenance metadata can include information about GAI model developers or creators of GAI content, \ndate/time of creation, location, modiﬁcations, and sources. Metadata can be tracked for text, images,'
 'Provenance Data Tracking Approaches \nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data \ninputs, metadata, and synthetic content. Provenance data tracking records the origin and history for \ndigital content, allowing its authenticity to be determined. It consists of techniques to record metadata \nas well as overt and covert digital watermarks on content. Data provenance refers to tracking the origin']","Tracking AI content's origin helps with authenticity and trust by providing information about the history and provenance of the content. Provenance data tracking can trace the origin and modifications of digital content, which assists in determining its authenticity. This transparency enables users to better understand the trustworthiness of AI systems, as they can access information about content creators, creation dates, locations, and any alterations made. By improving information integrity and upholding public trust, these mechanisms facilitate greater knowledge of both authentic and synthetic content.","Tracking the origin and history of AI-generated content through provenance data helps assess its authenticity and integrity, thereby improving trustworthiness in AI systems. It enables users to have better knowledge of the trustworthiness of both authentic and synthetic content, and when combined with organizational accountability mechanisms, it can trace negative outcomes back to their source, enhancing public trust.",1.0,1.0,1.0,0.999999999975,0.7736511217890703
16,How do we balance risk management and transparency in GAI governance?,"['Suggested Action \nGAI Risks \nGV-4.1-001 \nEstablish policies and procedures that address continual improvement processes \nfor GAI risk measurement. Address general risks associated with a lack of \nexplainability and transparency in GAI systems by using ample documentation and \ntechniques such as: application of gradient-based attributions, occlusion/term \nreduction, counterfactual prompts and prompt engineering, and analysis of'
 'AI Actor Tasks: Governance and Oversight \n \nGOVERN 1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based \non the organization’s risk tolerance. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.3-001 \nConsider the following factors when updating or deﬁning risk tiers for GAI: Abuses \nand impacts to information integrity; Dependencies between GAI and other IT or'
 'AI Actor Tasks: Governance and Oversight \n \nGOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other \ncontrols based on organizational risk priorities. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.4-001 \nEstablish policies and mechanisms to prevent GAI systems from generating \nCSAM, NCII or content that violates the law.  \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias \nand Homogenization;'
 'Harmful Bias Managed, Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable \n3. \nSuggested Actions to Manage GAI Risks \nThe following suggested actions target risks unique to or exacerbated by GAI. \nIn addition to the suggested actions below, AI risk management activities and actions set forth in the AI \nRMF 1.0 and Playbook are already applicable for managing GAI risks. Organizations are encouraged to']",I don't know.,The answer to given question is not present in context,0.0,0.0,1.0,0.0,0.1952036139761094
17,"What to consider for GAI's sustainability and societal impact, especially on data diversity and community input?","['29 \nMS-1.1-006 \nImplement continuous monitoring of GAI system impacts to identify whether GAI \noutputs are equitable across various sub-populations. Seek active and direct \nfeedback from aﬀected communities via structured feedback mechanisms or red-\nteaming to monitor and improve outputs.  \nHarmful Bias and Homogenization \nMS-1.1-007 \nEvaluate the quality and integrity of data used in training and the provenance of \nAI-generated content, for example by employing techniques like chaos'
 '20 \nGV-4.3-003 \nVerify information sharing and feedback mechanisms among individuals and \norganizations regarding any negative impact from GAI systems. \nInformation Integrity; Data \nPrivacy \nAI Actor Tasks: AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \n \nGOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those'
 'reporting, data protection, data privacy, or other laws. \nData Privacy; Human-AI \nConﬁguration; Information \nSecurity; Value Chain and \nComponent Integration; Harmful \nBias and Homogenization \nGV-6.2-004 \nEstablish policies and procedures for continuous monitoring of third-party GAI \nsystems in deployment. \nValue Chain and Component \nIntegration \nGV-6.2-005 \nEstablish policies and procedures that address GAI data redundancy, including \nmodel weights and other system artifacts.'
 'out by employees or third parties. Many of the actions cited above are relevant and options for \naddressing third-party considerations. \nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \nthe collection and use of third-party data for model inputs. Organizations may consider varying risk']","For GAI's sustainability and societal impact, especially on data diversity and community input, it is important to implement continuous monitoring of GAI system impacts to ensure outputs are equitable across various sub-populations. Additionally, seeking active and direct feedback from affected communities through structured feedback mechanisms or red-teaming is essential to monitor and improve outputs. Evaluating the quality and integrity of data used in training, including the provenance of AI-generated content, is also crucial. Furthermore, establishing policies and procedures for continuous monitoring of third-party GAI systems and addressing data redundancy are important considerations.","For GAI's sustainability and societal impact, it is important to assess the proportion of synthetic to non-synthetic training data to ensure it is not overly homogenous or GAI-produced. Additionally, documenting anticipated environmental impacts of model development, maintenance, and deployment is crucial. Gathering structured feedback about content provenance from operators, users, and impacted communities is also essential to actively seek feedback on generated content quality and potential biases, thereby ensuring diverse and inclusive content generation.",0.8888888888888888,0.937307081842687,0.25,0.9999999999666667,0.5891805050094376
18,What ensures fairness and oversight in automated systems?,"['safeguards against human bias, should be proportionate to the potential of the automated system to meaning\xad\nfully impact rights, opportunities, or access. Automated systems that have greater control over outcomes, \nprovide input to high-stakes decisions, relate to sensitive domains, or otherwise have a greater potential to \nmeaningfully impact rights, opportunities, or access should have greater availability (e.g., staffing) and over\xad\nsight of human consideration and fallback mechanisms.'
 'ensure that all people are treated fairly when automated systems are used. This includes all dimensions of their \nlives, from hiring to loan approvals, from medical treatment and payment to encounters with the criminal \njustice system. Ensuring equity should also go beyond existing guardrails to consider the holistic impact that \nautomated systems make on underserved communities and to institute proactive protections that support these \ncommunities. \n•'
 'WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nDemonstrate that the system protects against algorithmic discrimination \nIndependent evaluation. As described in the section on Safe and Effective Systems, entities should allow'
 'these existing practices, increase confidence in the use of automated systems, and protect the American public. Inno\xad\nvators deserve clear rules of the road that allow new ideas to flourish, and the American public deserves protections \nfrom unsafe outcomes. All can benefit from assurances that automated systems will be designed, tested, and consis\xad\ntently confirmed to work as intended, and that they will be proactively protected from foreseeable unintended harm\xad\nful outcomes. \n•']","Fairness and oversight in automated systems are ensured by implementing safeguards against human bias that are proportionate to the system's potential impact on rights, opportunities, or access. Additionally, automated systems that control significant outcomes or relate to sensitive domains should have increased availability of human oversight and fallback mechanisms. Proactive protections supporting underserved communities and independent evaluations to prevent algorithmic discrimination also contribute to fairness and oversight.","Fairness and oversight in automated systems are ensured through several expectations: implementing additional human oversight and safeguards, narrowly scoping data and inferences, tailoring systems to specific use cases, requiring human consideration before high-risk decisions, and providing meaningful access to examine the system. Additionally, reporting on the accessibility, timeliness, and effectiveness of human consideration and fallback is essential.",0.7142857142857143,0.9840068929980893,1.0,0.999999999975,0.41601507148250716
19,What factors to consider for GAI data privacy and compliance?,"['reporting, data protection, data privacy, or other laws. \nData Privacy; Human-AI \nConﬁguration; Information \nSecurity; Value Chain and \nComponent Integration; Harmful \nBias and Homogenization \nGV-6.2-004 \nEstablish policies and procedures for continuous monitoring of third-party GAI \nsystems in deployment. \nValue Chain and Component \nIntegration \nGV-6.2-005 \nEstablish policies and procedures that address GAI data redundancy, including \nmodel weights and other system artifacts.'
 'assessments, and alerting, dynamic risk assessments, and real-time reporting \ntools for monitoring third-party GAI risks; Consider policy adjustments across GAI \nmodeling libraries, tools and APIs, ﬁne-tuned models, and embedded tools; \nAssess GAI vendors, open-source or proprietary GAI tools, or GAI service \nproviders against incident or vulnerability databases. \nData Privacy; Human-AI \nConﬁguration; Information \nSecurity; Intellectual Property; \nValue Chain and Component'
 '47 \nAppendix A. Primary GAI Considerations \nThe following primary considerations were derived as overarching themes from the GAI PWG \nconsultation process. These considerations (Governance, Pre-Deployment Testing, Content Provenance, \nand Incident Disclosure) are relevant for voluntary use by any organization designing, developing, and \nusing GAI and also inform the Actions to Manage GAI risks. Information included about the primary'
 'some cases may include personal data. The use of personal data for GAI training raises risks to widely \naccepted privacy principles, including to transparency, individual participation (including consent), and \npurpose speciﬁcation. For example, most model developers do not disclose speciﬁc data sources on \nwhich models were trained, limiting user awareness of whether personally identiﬁably information (PII) \nwas trained on and, if so, how it was collected.']","The factors to consider for GAI data privacy and compliance include:

1. **Transparency**: The need to disclose specific data sources used for GAI training.
2. **Individual Participation**: Ensuring consent and awareness regarding the use of personal data.
3. **Purpose Specification**: Clearly defining the purposes for which personal data is used in GAI training.
4. **Compliance with Data Protection Laws**: Adhering to laws related to data protection and privacy.
5. **Continuous Monitoring**: Establishing policies and procedures for the continuous monitoring of third-party GAI systems.
6. **Data Redundancy**: Addressing GAI data redundancy, including model weights and other system artifacts.

These considerations are crucial in managing risks associated with the use of personal data in GAI systems.",The answer to given question is not present in context,1.0,0.995937349473715,1.0,0.249999999975,0.17627747889022138
